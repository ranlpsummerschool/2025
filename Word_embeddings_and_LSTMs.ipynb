{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6BGjZfU49eB"
   },
   "source": [
    "# **Word Embedding with Gensim**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0OByl12R3zL"
   },
   "source": [
    "The advancement of deep learning in Natural Language Processing is often attributed to the advent of word embeddings. Rather than using the words themselves as features, neural network methods typically take as input dense, relatively low-dimensional vectors that model the meaning and usage of a word.The concept of word embeddings gained prominence through models like Word2Vec, pioneered by Thomas Mikolov and his Google team. Subsequently, various other methods emerged, including GloVe and FastText embeddings. In this notebook, we'll delve into word embeddings using the original Word2Vec method, as implemented in the Gensim library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbrw1acZWU-e"
   },
   "source": [
    "# Training word embedding\n",
    "\n",
    "Training word embeddings with Gensim couldn't be easier. The only thing we need is a corpus of sentences in the language under investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"https://drive.google.com/uc?id=1UmvSodP7wo_L8OBu_9ImaLe5lVEVaiPX\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install spacy\n",
    "pip install scikit-learn\n",
    "pip install --upgrade gensim\n",
    "pip install pandas\n",
    "pip install matplotlib\n",
    "pip install scikit-learn\n",
    "pip install datasets\n",
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oTRTV0Qx1W9w"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import csv\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "class Corpus(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.nlp = spacy.blank(\"en\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, \"r\") as i:\n",
    "            reader = csv.reader(i, delimiter=\",\")\n",
    "            # tokenise the lower cased text using spacy tokeniser\n",
    "            for _, abstract in reader:\n",
    "                tokens = [t.text.lower() for t in self.nlp(abstract)]\n",
    "                yield tokens\n",
    "\n",
    "\n",
    "documents = Corpus(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9ua2CoYsBsT"
   },
   "source": [
    "When we train our word embeddings, gensim allows us to set a number of parameters. The most important of these are min_count, window, vector_size and sg:\n",
    "\n",
    "* **min_count** is the minimum frequency of the words in our corpus. For infrequent\n",
    "words, we just don't have enough information to train reliable word embeddings. It therefore makes sense to set this minimum frequency to at least 10. In these experiments, we'll set it to 100 to limit the size of our model even more.\n",
    "window is the number of words to the left and to the right that make up the context that word2vec will take into account.\n",
    "\n",
    "* **vector_size** is the dimensionality of the word vectors. This is generally between 100 and 1000. This dimensionality often forces us to make a trade-off: embeddings with a higher dimensionality are able to model more information, but also need more data to train.\n",
    "\n",
    "* **sg**: there are two algorithms to train word2vec: skip-gram and CBOW. Skip-gram tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. By default, Gensim uses CBOW (sg=0).\n",
    "\n",
    "\n",
    "We'll investigate the impact of some of these parameters later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OKLdAsZMvecA"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(documents, min_count=100, window=5, vector_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSO69ONLtfad"
   },
   "source": [
    "# Using word embeddings\n",
    "\n",
    "Let's take a look at the trained model. The word embeddings are on its wv attribute, and we can access them by the using the token as key. For example, here is the embedding for nlp, with the requested 100 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "boEiu5iNuA-D",
    "outputId": "c8ce8b3f-e2ca-40e1-fa5a-6f95d5451269"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.2049196e+00, -2.0399175e+00,  1.9319603e+00, -1.1644354e-01,\n",
       "        1.7962197e+00,  6.4193588e-01,  1.2741158e+00, -7.9043919e-01,\n",
       "       -5.4795825e-01,  1.5752438e+00, -1.2307194e-02, -3.3175652e+00,\n",
       "        6.7342067e-01, -4.6384311e-01, -5.6133050e-01,  9.0628380e-01,\n",
       "       -1.1305474e+00,  1.1876471e+00,  4.4961435e-01,  1.5232416e+00,\n",
       "        2.1306667e+00, -3.9594689e-01,  2.3165701e-01,  8.6593503e-01,\n",
       "       -2.4511082e+00, -1.4231625e+00,  6.9664586e-01,  1.1180078e+00,\n",
       "        1.1939554e+00, -1.0265014e+00,  7.2400504e-01,  1.1364397e+00,\n",
       "        4.1103932e-01,  1.1459923e+00,  1.0287075e+00, -1.1662725e+00,\n",
       "       -1.5545501e+00, -2.6318336e+00, -2.6789132e-01,  1.9964441e+00,\n",
       "       -1.2992334e+00, -2.4077046e+00,  2.4514947e+00,  3.3987838e-01,\n",
       "       -1.0033230e+00,  3.0160430e-01,  2.0661438e+00,  1.8518491e+00,\n",
       "       -6.1098385e-01, -2.7785852e+00,  2.0116267e+00,  1.9540480e+00,\n",
       "       -5.5057816e-02, -1.1721641e+00,  3.4381017e-01,  7.1585095e-01,\n",
       "       -9.6706666e-02,  1.0207353e+00,  4.4333172e+00,  4.3003019e-02,\n",
       "       -2.2857642e-01,  2.8700626e+00, -1.1190300e+00,  1.2144437e+00,\n",
       "       -9.3619215e-01,  6.2216210e-01, -6.1287326e-01, -2.3561630e+00,\n",
       "        1.2619425e+00, -7.0748389e-01, -1.7366862e+00, -2.3820863e+00,\n",
       "        6.6792351e-01,  2.0510723e-01, -7.1116680e-01,  2.7206177e-01,\n",
       "        1.2785672e+00, -7.5268114e-01, -1.6002836e+00,  2.9840404e-01,\n",
       "        1.0987378e+00,  2.4922948e+00,  7.4270875e-03,  6.2287605e-01,\n",
       "       -2.0733614e+00,  1.1178237e+00,  4.7245196e-01,  3.5234615e-01,\n",
       "       -3.6442075e+00, -3.3352652e-01,  7.9532981e-01,  1.0606564e+00,\n",
       "       -1.4017865e-01, -1.8337964e-01,  7.8383788e-02,  2.9247480e-03,\n",
       "       -3.6315623e-01, -1.1372749e+00,  1.0689587e-01, -1.5517837e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"nlp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sTkW9cLuDuW"
   },
   "source": [
    "We can also easily find the similarity between two words. Similarity is measured as the cosine between the two word embeddings, and therefore ranges between -1 and +1. The higher the cosine, the more similar two words are. As expected, the figures below show that nmt (neural machine translation) is closer to smt (statistical machine translation) than to ner (named entity recognition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uO6Rnq_iuBUj",
    "outputId": "125efcdd-4b6a-4ac2-e581-507430ba42f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5043842\n",
      "-0.08405011\n",
      "0.69067127\n",
      "0.39934808\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity(\"experiments\",\"results\"))\n",
    "print(model.wv.similarity(\"linguistics\",\"translation\"))\n",
    "\n",
    "# More specific example\n",
    "print(model.wv.similarity(\"nmt\", \"smt\"))\n",
    "print(model.wv.similarity(\"nmt\", \"ner\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlZfhx1ru8kN"
   },
   "source": [
    "In a similar vein, we can find the words that are most similar to a target word. The words with the most similar embedding to bert are all semantically related to it: other types of pretrained models such as roberta, mbert, xlm, as well as the more general model type BERT represents (transformer and transformers), and more generally related words (pretrained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "57EVfU_tu7TZ",
    "outputId": "605320e5-6f84-4768-d6dc-7c511fe9fce1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('roberta', 0.7877159714698792),\n",
       " ('transformer', 0.7542476654052734),\n",
       " ('elmo', 0.7362399101257324),\n",
       " ('transformers', 0.7179213166236877),\n",
       " ('pretrained', 0.7090664505958557),\n",
       " ('gpt-2', 0.6605957746505737),\n",
       " ('mbert', 0.6548022627830505),\n",
       " ('xlm', 0.6529796123504639),\n",
       " ('xlnet', 0.6369994878768921),\n",
       " ('lstm', 0.6300441026687622)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"bert\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2gy0Bbo6NXz",
    "outputId": "01b2e8ad-cfe1-4577-f764-10b1da03758e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paraphrases', 0.7356393337249756),\n",
       " ('documents', 0.7144497036933899),\n",
       " ('strings', 0.696727991104126),\n",
       " ('texts', 0.6827948689460754),\n",
       " ('keyphrases', 0.6799679398536682),\n",
       " ('tokens', 0.6754116415977478),\n",
       " ('words', 0.6572448015213013),\n",
       " ('phrases', 0.6515894532203674),\n",
       " ('poems', 0.6500973105430603),\n",
       " ('paragraphs', 0.6317392587661743)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"sentences\", topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5fGObHav9XM"
   },
   "source": [
    "Interestingly, we can look for words that are similar to a set of words and dissimilar to another set of words at the same time. This allows us to look for analogies of the type BERT is to a transformer like an LSTM is to .... Our embedding model correctly predicts that LSTMs are a type of RNN, just like BERT is a particular type of transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvsSy-HTuBd3",
    "outputId": "be7300f0-4eb6-425d-a280-36a085ae95d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rnn', 0.8247852921485901)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"transformer\", \"lstm\"], negative=[\"bert\"], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vo6CbbOywE3B"
   },
   "source": [
    "Similarly, we can also zoom in on one of the meanings of ambiguous words. For example, in NLP tree has a very specific meaning, which is obvious from its nearest neighbours constituency, parse, dependency and syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WN-Cm-2Ev5AJ",
    "outputId": "a165e5e2-57dc-4e4f-a0cd-48df3409f26e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trees', 0.7817927002906799),\n",
       " ('constituency', 0.7085326910018921),\n",
       " ('recursive', 0.6886435151100159),\n",
       " ('parse', 0.6862645149230957),\n",
       " ('formalism', 0.6298339366912842),\n",
       " ('dependency', 0.6251062750816345),\n",
       " ('constituent', 0.6246699094772339),\n",
       " ('path', 0.616413950920105),\n",
       " ('hierarchical', 0.607570469379425),\n",
       " ('kernels', 0.6073637008666992)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"tree\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jxm5HTKkwNiC"
   },
   "source": [
    "However, if we specify we're looking for words that are similar to tree, but dissimilar to syntax, suddenly its standard meaning takes over, and forest crops up in its nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW8PU7uOwSCi",
    "outputId": "5072960d-335f-4d0d-f3bc-7dfe4f54c4a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('modified', 0.4574902653694153),\n",
       " ('bayes', 0.4566059112548828),\n",
       " ('greedy', 0.4538378119468689),\n",
       " ('forest', 0.4479946196079254),\n",
       " ('random', 0.4259032607078552),\n",
       " ('logistic', 0.4248599112033844),\n",
       " ('feed', 0.4096589982509613),\n",
       " ('nearest', 0.4025936424732208),\n",
       " ('softmax', 0.39892685413360596),\n",
       " ('stochastic', 0.3971480429172516)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[\"tree\"], negative=[\"syntax\"], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhqKa8WiwNd5"
   },
   "source": [
    "Finally, we can present the word2vec model with a list of words and ask it to identify the odd one out. It then uses the word embeddings to identify the word that is least similar to the other ones. For example, in the list lstm cnn gru svm transformer, it correctly identifies svm as the only non-neural model. In the list bert word2vec gpt-2 roberta xlnet, it correctly singles out word2vec as the only non-transormer model. In word2vec bert glove fasttext elmo, bert is singled out as the only transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6K9XeZiwo6t",
    "outputId": "b0d8a791-5b2f-4c96-853f-6457b3082627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm\n",
      "word2vec\n",
      "bert\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(\"lstm cnn gru svm transformer\".split()))\n",
    "print(model.wv.doesnt_match(\"bert word2vec gpt-2 roberta xlnet\".split()))\n",
    "print(model.wv.doesnt_match(\"word2vec bert glove fasttext elmo\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPs1iQnLz08H"
   },
   "source": [
    "# Exploring hyperparameters\n",
    "\n",
    "We mentioned above there are a number of parameters we can set when training our embeddings. Let's investigate the impact some of these have on the result. Quantifying the quality of embeddings is a hard task. There exist quite a few data sets for evaluating the quality of English embeddings, but this is not the case for other languages or very specialized domains, such as NLP. Moreover, it's unclear what information good embeddings should capture. Should they model syntactic information as well as semantic knowledge? Should they capture semantic similarity, or merely topical relatedness? Often, the answer depends on the end task you want to use the embeddings for.\n",
    "\n",
    "Here we'll use a simple method for evaluating our embeddings. We'll count how many times two nearest neighbours in the vector space have the same part of speech. After all, if our embeddings model similarity (and not just relatedness) in meaning, we expect a noun to have another noun as nearest neighbour, and the same for verbs, adjectives, and so on.\n",
    "\n",
    "First we'll use spaCy to determine the part of speech of all the words in our vocabulary. Note that our evaluation metric does rely on the quality of spaCy's part-of-speech tagging, which may not be very accurate for low-frequency words out of context. Nevertheless, we'll assume it's good enough for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy installation path: /venv/main/lib/python3.12/site-packages/spacy/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Get spaCy's installation path\n",
    "spacy_path = spacy.__file__\n",
    "print(f\"SpaCy installation path: {spacy_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "752be37cf0944f4990c32b166fa06e48",
      "5ea7116b06f146609a48c001992626c5",
      "a07f4a6afc78467296d16c0e5ec19a0d",
      "04f3e4b5ed5649dc9ac1e71d8a42e061",
      "45ac3b8efacb45c1bade437387336706",
      "e1b38d7456f6463e88a24710b0f70c96",
      "afe55d2e6050455e8bab375ef9b0892a",
      "300fee4e0335489e88c28bea1f82fe78",
      "cd3a0528eaa949b2a10a80d33792b29f",
      "dcbcadd277d24b3787bcd7e7ee509598",
      "52f7e15a1fc8451599cdadbebb89b17f"
     ]
    },
    "id": "nUZY11rg0Yev",
    "outputId": "e6a6821e-5901-4d0c-cf20-32837832addc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f58e4e1b61641d5aec82030a82b4dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "word2pos = {}\n",
    "for word in tqdm(model.wv.key_to_index):\n",
    "    word2pos[word] = nlp(word)[0].pos_\n",
    "\n",
    "word2pos[\"translation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U0St6RWz1Fm"
   },
   "source": [
    "\n",
    "Then we write a simple method that takes a model and looks up the nearest neighbour to every word in its vocabulary. It returns the number of times this nearest neighbour has the same part of speech: a percentage we'll call the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "60d1b0414e9f4a6a8850723200dabb2b",
      "9920a5c74fb24b6f8f34eb5768f493fe",
      "7124b9decb764c7e8af37dfad890c624",
      "b765ebe2afb6411386be6f6db816bd93",
      "6951096e00394c98a0ecd2c9351f308e",
      "568cf4b4dea34255809719cd71b5e770",
      "5e47c636b5394113a03f9dbe01a268f3",
      "437407bb5bb74bf380566995c2870cb9",
      "7f348c16ad24431caf9bc5d7781eca4a",
      "720ec6b6af9c45ecac219eb8e831a086",
      "2657ff957f5e4c12b31c5004048fa8a5"
     ]
    },
    "id": "7kAo7jHj0ljM",
    "outputId": "3676b0de-05d1-4506-a84b-9dc1fcd470b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cda7363a9184a9cac2377445c8cd5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6479509519199742"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(model, word2pos):\n",
    "    same = 0\n",
    "    for word in tqdm(model.wv.key_to_index):\n",
    "        most_similar = model.wv.similar_by_word(word, topn=1)[0][0]\n",
    "        if word2pos[most_similar] == word2pos[word]:\n",
    "            same += 1\n",
    "    return same/len(model.wv.key_to_index)\n",
    "\n",
    "evaluate(model, word2pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Elre9UN90kQ_"
   },
   "source": [
    "Now we vary some of the settings we introduced above. In particular we're interested in the influence of embedding size (the dimensionality of the trained embeddings), and the size of the context window. We vary the embedding size between 100, 200 and 300,and the context window between 2, 5 and 10. This means we'll train 9 models in total, which obviously takes a bit of time. Feel free to go grab a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qKWTAuJv02Ft",
    "outputId": "51b83d87-954e-4046-bb53-c1bfdd8d3477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100 Window: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a9c3d6e4dc4750b125142f756f63ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18933/2963030588.py:11: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df[size][window] = acc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100 Window: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c487a4570c0d4973b70fae7de9fe3b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 100 Window: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e99fb4d3634b3a8f4ef65bcf1eab54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 200 Window: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268f4679806c4d308ccf58cc80528387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 200 Window: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69217301dc4e408d9cf7401203690ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 200 Window: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e029a1ca1d48e1a86d0df8d1423137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 300 Window: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05e19636f5145fb97a45806d5c09ca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 300 Window: 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "528941c3456f407cabba1b6d8059a77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 300 Window: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d7adfb64b747159b1fe29d84812fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.680865</td>\n",
       "      <td>0.679897</td>\n",
       "      <td>0.674088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.655373</td>\n",
       "      <td>0.644079</td>\n",
       "      <td>0.643756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.616651</td>\n",
       "      <td>0.622459</td>\n",
       "      <td>0.623104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         100       200       300\n",
       "2   0.680865  0.679897  0.674088\n",
       "5   0.655373  0.644079  0.643756\n",
       "10  0.616651  0.622459  0.623104"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [100, 200, 300]\n",
    "windows = [2,5,10]\n",
    "\n",
    "df = pd.DataFrame(index=windows, columns=sizes)\n",
    "\n",
    "for size in sizes:\n",
    "    for window in windows:\n",
    "        print(\"Size:\", size, \"Window:\", window)\n",
    "        model = gensim.models.Word2Vec(documents, min_count=100, window=window, vector_size=size)\n",
    "        acc = evaluate(model, word2pos)\n",
    "        df[size][window] = acc\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEIsOiQTz1IK"
   },
   "source": [
    "Although the accuracies of all models are very similar, the results do show some interesting patterns.\n",
    "\n",
    "First, it looks like smaller contexts work better than larger ones. This is logical, as our evaluation metric is a syntactic one: the closest context words contain much more useful information about the part of speech of a word than those further away.\n",
    "\n",
    "Second, higher-dimensional word embeddings do not necessarily work better than lower-dimensional ones. This may sound counter-intuitive, as higher-dimensional embeddings are able to capture more information. Still, larger embeddings also require more data, while we're using a pretty small corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "N9ty2KWH0jXb",
    "outputId": "fd763959-11c5-4440-f801-3007a2deaf22"
   },
   "outputs": [],
   "source": [
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djk6kVVkz1Lp"
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "Word embeddings are one of the most exciting trends on Natural Language Processing since the 2000s. They allow us to model the meaning and usage of a word, and discover words that behave similarly. This is crucial for the generalization capacity of many machine learning models. Moving from raw strings to embeddings allows them to generalize across words that have a similar meaning, and discover patterns that had previously escaped them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQCo9tniVkH3"
   },
   "source": [
    "### **Simple Task for you**\n",
    "Download a pretrained word2vec wordembedding and perform above similarity checks and evaluate and compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITb9M2iDAjsl"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQa8jTv89eSo"
   },
   "source": [
    "# ***LSTM & CNN for classification***\n",
    "This part describes how to implement LSTM models for text binary classification using tensorflow and keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYgww9gx9rTH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 500\n",
    "VALIDATION_SIZE = 0.2\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# Method 1: Convert to pandas first (recommended)\n",
    "df_train = imdb['train'].to_pandas()\n",
    "\n",
    "# Split the data\n",
    "train_split, validation_split = train_test_split(\n",
    "    df_train, \n",
    "    test_size=VALIDATION_SIZE, \n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Extract as lists\n",
    "train_txt = train_split['text'].tolist()\n",
    "train_lbl = train_split['label'].tolist()\n",
    "validation_txt = validation_split['text'].tolist()\n",
    "validation_lbl = validation_split['label'].tolist()\n",
    "\n",
    "test_txt = imdb['test']['text']\n",
    "test_lbl = imdb['test']['label']\n",
    "\n",
    "print(f\"Training samples: {len(train_txt)}\")\n",
    "print(f\"Validation samples: {len(validation_txt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhtEReZj93V_"
   },
   "source": [
    "## Vectorise the dataset and build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n84k8Pg_9363"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Now create TensorFlow dataset\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_txt).batch(BATCH_SIZE)\n",
    "\n",
    "# Continue with your vectorizer\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE, \n",
    "    output_sequence_length=MAX_LENGTH\n",
    ")\n",
    "vectorizer.adapt(text_ds)\n",
    "\n",
    "print(\"Vectorizer adapted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANCZq4hk978n"
   },
   "outputs": [],
   "source": [
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pbFhVlS9-bu"
   },
   "outputs": [],
   "source": [
    "output = vectorizer([[\"You are welcome to the RANLP conference\"]])\n",
    "output.numpy()[0, :8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48eP0y2y-Af_"
   },
   "source": [
    "## Download embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown \"https://drive.google.com/uc?id=12tAq-AbroB9hHwi597lIyjktr3o62w8o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04mP9Enz-BQ_"
   },
   "outputs": [],
   "source": [
    "# !wget https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00t74lRz-Ih_"
   },
   "source": [
    "## Create word index and embeddings index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_Y53HzC-JRP"
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))\n",
    "\n",
    "path_to_glove_file = 'glove.6B.100d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdGuPD1H-MuW"
   },
   "source": [
    "## Build embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dP9vx3uk-PLH"
   },
   "outputs": [],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY6TlgDW-Rwn"
   },
   "source": [
    "## Create LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKifPYqw-SPm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    name='embeddings'\n",
    ")\n",
    "input = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"input\")\n",
    "x = embedding_layer(input)\n",
    "x = layers.LSTM(128, name=\"lstm_1\",return_sequences=True)(x)\n",
    "x = layers.LSTM(128, name=\"lstm_2\")(x)\n",
    "output = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"dense_predictions\")(x)\n",
    "model = keras.Model(inputs=input, outputs=output, name=\"lstm_model\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oO34-vti-YH3"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlU0Ee5J-Y3H"
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_txt])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in validation_txt])).numpy()\n",
    "\n",
    "y_train = np.array(train_lbl)\n",
    "y_val = np.array(validation_lbl)\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "optimiser = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "# optimiser = keras.optimizers.SGD(learning_rate=LEARNING_RATE)\n",
    "# optimiser = keras.optimizers.RMSprop(learning_rate=LEARNING_RATE)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=optimiser, metrics=[\"accuracy\"],\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=256, epochs=3, validation_data=(x_val, y_val))\n",
    "\n",
    "x_test = vectorizer(np.array([[s] for s in test_txt])).numpy()\n",
    "y_test = np.array(test_lbl)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9nHllw2-dj-"
   },
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "naxIz7fZ-eQW"
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2,min_delta=0.001)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=5, validation_data=(x_val, y_val),callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yS0Dmlbg-hpu"
   },
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NZPEBpb-hDU"
   },
   "outputs": [],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\", name=\"text_input\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "sample_text = tf.constant([\"I like this movie\"], dtype=tf.string)\n",
    "\n",
    "probabilities = end_to_end_model.predict(sample_text)\n",
    "print(\"Probabilities:\", probabilities)\n",
    "print(\"Predicted class:\", np.argmax(probabilities[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_no5LNeE-mmm"
   },
   "source": [
    "# Bi-LSTM\n",
    "\n",
    "Change the above model to a Bi-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuygiPxH-nIz"
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04f3e4b5ed5649dc9ac1e71d8a42e061": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcbcadd277d24b3787bcd7e7ee509598",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_52f7e15a1fc8451599cdadbebb89b17f",
      "value": " 3099/3099 [00:22&lt;00:00, 157.05it/s]"
     }
    },
    "2657ff957f5e4c12b31c5004048fa8a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "300fee4e0335489e88c28bea1f82fe78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "437407bb5bb74bf380566995c2870cb9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45ac3b8efacb45c1bade437387336706": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52f7e15a1fc8451599cdadbebb89b17f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "568cf4b4dea34255809719cd71b5e770": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e47c636b5394113a03f9dbe01a268f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ea7116b06f146609a48c001992626c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1b38d7456f6463e88a24710b0f70c96",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_afe55d2e6050455e8bab375ef9b0892a",
      "value": "100%"
     }
    },
    "60d1b0414e9f4a6a8850723200dabb2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9920a5c74fb24b6f8f34eb5768f493fe",
       "IPY_MODEL_7124b9decb764c7e8af37dfad890c624",
       "IPY_MODEL_b765ebe2afb6411386be6f6db816bd93"
      ],
      "layout": "IPY_MODEL_6951096e00394c98a0ecd2c9351f308e"
     }
    },
    "6951096e00394c98a0ecd2c9351f308e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7124b9decb764c7e8af37dfad890c624": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_437407bb5bb74bf380566995c2870cb9",
      "max": 3099,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f348c16ad24431caf9bc5d7781eca4a",
      "value": 3099
     }
    },
    "720ec6b6af9c45ecac219eb8e831a086": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "752be37cf0944f4990c32b166fa06e48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ea7116b06f146609a48c001992626c5",
       "IPY_MODEL_a07f4a6afc78467296d16c0e5ec19a0d",
       "IPY_MODEL_04f3e4b5ed5649dc9ac1e71d8a42e061"
      ],
      "layout": "IPY_MODEL_45ac3b8efacb45c1bade437387336706"
     }
    },
    "7f348c16ad24431caf9bc5d7781eca4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9920a5c74fb24b6f8f34eb5768f493fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_568cf4b4dea34255809719cd71b5e770",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5e47c636b5394113a03f9dbe01a268f3",
      "value": "100%"
     }
    },
    "a07f4a6afc78467296d16c0e5ec19a0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_300fee4e0335489e88c28bea1f82fe78",
      "max": 3099,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cd3a0528eaa949b2a10a80d33792b29f",
      "value": 3099
     }
    },
    "afe55d2e6050455e8bab375ef9b0892a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b765ebe2afb6411386be6f6db816bd93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_720ec6b6af9c45ecac219eb8e831a086",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2657ff957f5e4c12b31c5004048fa8a5",
      "value": " 3099/3099 [00:00&lt;00:00, 3347.73it/s]"
     }
    },
    "cd3a0528eaa949b2a10a80d33792b29f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dcbcadd277d24b3787bcd7e7ee509598": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1b38d7456f6463e88a24710b0f70c96": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
